{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcdMAn7muulO",
        "outputId": "f6afecfa-a359-47c3-91e7-e9fb33a0d669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd\n",
        "%cd /content/gdrive/'My Drive'/EvalIndQurNERDataset/code\n",
        "%pwd\n",
        "%ls -la"
      ],
      "metadata": {
        "id": "IticzSjRwhJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==2.4.1"
      ],
      "metadata": {
        "id": "mloyeHhj6PyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import BertConfig, BertTokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from modules.word_classification import BertForWordClassification\n",
        "from utils.forward_fn import forward_word_classification\n",
        "from utils.metrics import ner_metrics_fn\n",
        "from utils.data_utils import IndQurNerDataset"
      ],
      "metadata": {
        "id": "uiXKXxE4x7g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "t7s1TLGVE1UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_subword_tokenize(sentence, tokenizer):\n",
        "    # Add CLS token\n",
        "    subwords = [tokenizer.cls_token_id]\n",
        "    subword_to_word_indices = [-1] # For CLS\n",
        "\n",
        "    # Add subwords\n",
        "    for word_idx, word in enumerate(sentence):\n",
        "        subword_list = tokenizer.encode(word, add_special_tokens=False)\n",
        "        subword_to_word_indices += [word_idx for i in range(len(subword_list))]\n",
        "        subwords += subword_list\n",
        "\n",
        "    # Add last SEP token\n",
        "    subwords += [tokenizer.sep_token_id]\n",
        "    subword_to_word_indices += [-1]\n",
        "\n",
        "    return subwords, subword_to_word_indices"
      ],
      "metadata": {
        "id": "eCylQ7jPFVrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# common functions\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    \n",
        "def count_param(module, trainable=False):\n",
        "    if trainable:\n",
        "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "    else:\n",
        "        return sum(p.numel() for p in module.parameters())\n",
        "    \n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def metrics_to_string(metric_dict):\n",
        "    string_list = []\n",
        "    for key, value in metric_dict.items():\n",
        "        string_list.append('{}:{:.2f}'.format(key, value))\n",
        "    return ' '.join(string_list)\n",
        "\n",
        "# Load Tokenizer and Config\n",
        "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
        "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p1')\n",
        "config.num_labels = IndQurNerDataset.NUM_LABELS\n",
        "\n",
        "# Instantiate model\n",
        "model = BertForWordClassification.from_pretrained('indobenchmark/indobert-base-p1', config=config)\n",
        "w2i, i2w = IndQurNerDataset.LABEL2INDEX, IndQurNerDataset.INDEX2LABEL"
      ],
      "metadata": {
        "id": "RGef2wfN2PT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning process"
      ],
      "metadata": {
        "id": "9us-ZJ8_JYeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_path = 'datasets/train.txt'\n",
        "valid_dataset_path = 'datasets/dev.txt'\n",
        "test_dataset_path = 'datasets/test.txt'\n",
        "\n",
        "train_dataset = IndQurNerDataset(train_dataset_path, tokenizer, lowercase=True)\n",
        "valid_dataset = IndQurNerDataset(valid_dataset_path, tokenizer, lowercase=True)\n",
        "test_dataset = IndQurNerDataset(test_dataset_path, tokenizer, lowercase=True)\n",
        "\n",
        "train_loader = NerDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=True)  \n",
        "valid_loader = NerDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=False)  \n",
        "test_loader = NerDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=False)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "model = model.cuda()\n",
        "# Train\n",
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    torch.set_grad_enabled(True)\n",
        " \n",
        "    total_train_loss = 0\n",
        "    list_hyp, list_label = [], []\n",
        "\n",
        "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
        "    for i, batch_data in enumerate(train_pbar):\n",
        "        # Forward model\n",
        "        loss, batch_hyp, batch_label = forward_word_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "\n",
        "        # Update model\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tr_loss = loss.item()\n",
        "        total_train_loss = total_train_loss + tr_loss\n",
        "\n",
        "        # Calculate metrics\n",
        "        list_hyp += batch_hyp\n",
        "        list_label += batch_label\n",
        "\n",
        "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n",
        "            total_train_loss/(i+1), get_lr(optimizer)))\n",
        "\n",
        "    # Calculate train metric\n",
        "    metrics = ner_metrics_fn(list_hyp, list_label)\n",
        "    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
        "        total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n",
        "\n",
        "    # Evaluate on validation\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "    \n",
        "    total_loss, total_correct, total_labels = 0, 0, 0\n",
        "    list_hyp, list_label = [], []\n",
        "\n",
        "    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
        "    for i, batch_data in enumerate(pbar):\n",
        "        batch_seq = batch_data[-1]        \n",
        "        loss, batch_hyp, batch_label = forward_word_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "        None\n",
        "        # Calculate total loss\n",
        "        valid_loss = loss.item()\n",
        "        total_loss = total_loss + valid_loss\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        list_hyp += batch_hyp\n",
        "        list_label += batch_label\n",
        "        metrics = ner_metrics_fn(list_hyp, list_label)\n",
        "\n",
        "        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
        "        \n",
        "    metrics = ner_metrics_fn(list_hyp, list_label)\n",
        "    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
        "        total_loss/(i+1), metrics_to_string(metrics)))"
      ],
      "metadata": {
        "id": "dtkpAdr9GU3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the fine-tuned model on test set\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "total_loss, total_correct, total_labels = 0, 0, 0\n",
        "list_hyp, list_label = [], []\n",
        "\n",
        "pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
        "for i, batch_data in enumerate(pbar):\n",
        "    _, batch_hyp, batch_label = forward_word_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "    list_hyp += batch_hyp\n",
        "    list_label += batch_label\n",
        "\n",
        "# Save prediction\n",
        "df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
        "df.to_csv('pred.txt', index=False)\n",
        "\n",
        "print(df)\n",
        "\n",
        "metrics = ner_metrics_fn(list_hyp, list_label)\n",
        "print(metrics_to_string(metrics))"
      ],
      "metadata": {
        "id": "nDCQ4sp18pmk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}